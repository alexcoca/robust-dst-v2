{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%cd ..\n",
    "%pwd"
   ],
   "id": "200f60cddc16bc3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "from robust_dst.evaluation import get_metrics\n",
    "from robust_dst.scoring_utils import flatten_metrics_dict, setup_sgd_evaluation, setup_sgd_evaluator_inputs\n",
    "from robust_dst.utils import (\n",
    "    infer_data_version_from_path,\n",
    "    infer_schema_variant_from_path,\n",
    ")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ],
   "id": "28e57ca876ee9e80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "first_file_path = \"data/raw/original/test/dialogues_001.json\"\n",
    "hyp_path = \"experiments/metrics_and_dialogues_sdt_best.json\"\n",
    "\n",
    "with open(first_file_path, \"r\") as file:\n",
    "    refs = json.load(file)\n",
    "\n"
   ],
   "id": "b1241efa206ebf48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# with open(hyp_path, \"r\") as file:\n",
    "#     hyps = json.load(file)\n",
    "\n",
    "\n",
    "# Read everything in \"experiments/sdt\" folder\n",
    "# Take all files which start with dialogues_ and end with .json\n",
    "# Read them and convert them to a list of dictionaries, joining them all together to a single dictionary\n",
    "# called hyps\n",
    "hyps = []\n",
    "hyp_dir = Path(\"experiments/pytod\")\n",
    "for file_path in hyp_dir.glob(\"dialogues_*.json\"):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "        hyps.extend(data)\n",
    "\n",
    "hyps = {d[\"dialogue_id\"]: d for d in hyps}"
   ],
   "id": "27cb512ce2173037",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sgd_evaluator_inputs = setup_sgd_evaluator_inputs(Path('data/raw/original/test'))",
   "id": "e33892e613f0e0b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_metrics_aggregate, per_frame_metric = get_metrics(\n",
    "    dataset_ref=sgd_evaluator_inputs[\"dataset_ref\"],\n",
    "    dataset_hyp=hyps,\n",
    "    service_schemas=sgd_evaluator_inputs[\"eval_services\"],\n",
    "    in_domain_services=sgd_evaluator_inputs[\"in_domain_services\"],\n",
    "    fuzzy_threshold=0.8\n",
    ")"
   ],
   "id": "90a5fb6909f81bd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "services = [\"#ALL_SERVICES\", \"#SEEN_SERVICES\", \"#UNSEEN_SERVICES\"]\n",
    "metrics = [\"joint_goal_accuracy\", \"consistency_adjusted_joint_goal_accuracy\", \"correct_turns\"]\n",
    "for service in services:\n",
    "    for metric in metrics:\n",
    "        print(f\"{service}/{metric}: {all_metrics_aggregate[service][metric]}\")"
   ],
   "id": "95d6bdfaf4862c19",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
